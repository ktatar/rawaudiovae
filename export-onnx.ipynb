{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1gkDKBGeaGH3"
   },
   "source": [
    "# rawaudiovae"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset and pretrained models can be found here: \n",
    "\n",
    "https://drive.google.com/file/d/1e_X2Ir26iypSdSa6pRCJBy2q5t9zXFBb/view?usp=drive_link\n",
    "\n",
    "Please download this folder and unzip it under the ./content folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "P-4JbrvN_UVe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kivanc\\anaconda3\\envs\\torch2x\\lib\\site-packages\\torchaudio\\functional\\functional.py:1458: UserWarning: \"kaiser_window\" resampling method name is being deprecated and replaced by \"sinc_interp_kaiser\" in the next release. The default behavior remains unchanged.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torchaudio\n",
    "Resample = torchaudio.transforms.Resample(44100, 48000, resampling_method='kaiser_window')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "    my_cuda = 1\n",
    "else: \n",
    "    device = 'cpu'\n",
    "    my_cuda = 0\n",
    "    \n",
    "Resample = Resample.to(device)\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy import interpolate as sp_interpolate\n",
    "import json\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import soundfile as sf\n",
    "# import sounddevice as sd\n",
    "import configparser\n",
    "import random\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zR4gkZoSsrbG"
   },
   "outputs": [],
   "source": [
    "sampling_rate = 44100\n",
    "sr = sampling_rate\n",
    "\n",
    "hop_length = 128\n",
    "\n",
    "segment_length = 1024\n",
    "n_units = 2048\n",
    "latent_dim = 256\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "audio_fold = Path(r'./content/2022-zkm-workshop/ltsp/erokia/audio')\n",
    "audio = audio_fold\n",
    "lts_audio_files = [f for f in audio_fold.glob('*.wav')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sSInHDvAu2_-",
    "outputId": "ebab8eac-e201-4b43-874c-ca92bc0ba06b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1139"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Following should give you more than 0. Otherwise, the dataset is not in the right place. Please make sure that the following folder is there: rawaudiovae/content/2022-zkm-workshop\n",
    "\n",
    "len(lts_audio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kdUWpV22trrp"
   },
   "outputs": [],
   "source": [
    "# Models \n",
    "\n",
    "class raw_VAE(nn.Module):\n",
    "  def __init__(self, segment_length, n_units, latent_dim):\n",
    "    super(raw_VAE, self).__init__()\n",
    "\n",
    "    self.segment_length = segment_length\n",
    "    self.n_units = n_units\n",
    "    self.latent_dim = latent_dim\n",
    "    \n",
    "    self.fc1 = nn.Linear(segment_length, n_units)\n",
    "    self.fc21 = nn.Linear(n_units, latent_dim)\n",
    "    self.fc22 = nn.Linear(n_units, latent_dim)\n",
    "    self.fc3 = nn.Linear(latent_dim, n_units)\n",
    "    self.fc4 = nn.Linear(n_units, segment_length)\n",
    "\n",
    "  def encode(self, x):\n",
    "      h1 = F.relu(self.fc1(x))\n",
    "      return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "  def reparameterize(self, mu, logvar):\n",
    "      std = torch.exp(0.5*logvar)\n",
    "      eps = torch.randn_like(std)\n",
    "      return mu + eps*std\n",
    "\n",
    "  def decode(self, z):\n",
    "      h3 = F.relu(self.fc3(z))\n",
    "      return F.tanh(self.fc4(h3))\n",
    "\n",
    "  def forward(self, x):\n",
    "      mu, logvar = self.encode(x.view(-1, self.segment_length))\n",
    "      z = self.reparameterize(mu, logvar)\n",
    "      return self.decode(z), mu, logvar\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar, kl_beta, segment_length):\n",
    "  recon_loss = F.mse_loss(recon_x, x.view(-1, segment_length))\n",
    "\n",
    "  # see Appendix B from VAE paper:\n",
    "  # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "  # https://arxiv.org/abs/1312.6114\n",
    "  # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "  KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "  return recon_loss + ( kl_beta * KLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6auOc8Zqtx-r"
   },
   "outputs": [],
   "source": [
    "# Datasets \n",
    "\n",
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    This is the main class that calculates the spectrogram and returns the\n",
    "    spectrogram, audio pair.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, audio_np, segment_length, sampling_rate, hop_size, transform=None):\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.segment_length = segment_length\n",
    "        self.hop_size = hop_size\n",
    "        \n",
    "        if segment_length % hop_size != 0:\n",
    "            raise ValueError(\"segment_length {} is not a multiple of hop_size {}\".format(segment_length, hop_size))\n",
    "\n",
    "        if len(audio_np) % hop_size != 0:\n",
    "            num_zeros = hop_size - (len(audio_np) % hop_size)\n",
    "            audio_np = np.pad(audio_np, (0, num_zeros), 'constant', constant_values=(0,0))\n",
    "\n",
    "        self.audio_np = audio_np\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # Take segment\n",
    "        seg_start = index * self.hop_size\n",
    "        seg_end = (index * self.hop_size) + self.segment_length\n",
    "        sample = self.audio_np[ seg_start : seg_end ]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.audio_np) // self.hop_size) - (self.segment_length // self.hop_size) + 1\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return torch.from_numpy(sample)\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    This is the main class that calculates the spectrogram and returns the\n",
    "    spectrogram, audio pair.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, audio_np, segment_length, sampling_rate, transform=None):\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.segment_length = segment_length\n",
    "        \n",
    "        if len(audio_np) % segment_length != 0:\n",
    "            num_zeros = segment_length - (len(audio_np) % segment_length)\n",
    "            audio_np = np.pad(audio_np, (0, num_zeros), 'constant', constant_values=(0,0))\n",
    "\n",
    "        self.audio_np = audio_np\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # Take segment\n",
    "        seg_start = index * self.segment_length\n",
    "        seg_end = (index * self.segment_length) + self.segment_length\n",
    "        sample = self.audio_np[ seg_start : seg_end ]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_np) // self.segment_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZAK-ZayJs37R",
    "outputId": "78b63eea-6f03-4ea1-dff1-8217a12ad6a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_VAE(\n",
       "  (fc1): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "  (fc21): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  (fc22): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=2048, bias=True)\n",
       "  (fc4): Linear(in_features=2048, out_features=1024, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = torch.load(Path(r'./content/2022-zkm-workshop/nospectral/erokia/spectralvae/run-000/checkpoints/ckpt_00500'), map_location=torch.device(device))\n",
    "if my_cuda:\n",
    "    raw_model = raw_VAE(segment_length, n_units, latent_dim).to(device)\n",
    "else:\n",
    "    raw_model = raw_VAE(segment_length, n_units, latent_dim)\n",
    "raw_model.load_state_dict(state['state_dict'])\n",
    "raw_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Create onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_input = (torch.randn(1024).to(device))\n",
    "torch.onnx.export(raw_model, dummy_input, \"rawaudiovae.onnx\", verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "This code is built within the following research residency:\n",
    "\n",
    "https://kivanctatar.com/Coding-Latent-No-1\n",
    "\n",
    "This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program â€“ Humanities and Society (WASP-HS) funded by the Marianne and Marcus Wallenberg Foundation and the Marcus and Amalia Wallenberg Foundation."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "GLhSNjjgERrr",
    "VaOjyqxHg-FP",
    "vzmoFaAunkIx",
    "eH70BnNZoD2V"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
