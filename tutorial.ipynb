{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Auw-8WiPo0b7"
      },
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1gkDKBGeaGH3"
      },
      "source": [
        "# rawaudiovae"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A dataset and pretrained models can be found here: \n",
        "\n",
        "https://drive.google.com/file/d/1e_X2Ir26iypSdSa6pRCJBy2q5t9zXFBb\n",
        "\n",
        "Please download this folder and unzip it under the ./content folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-4JbrvN_UVe"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import torchaudio\n",
        "Resample = torchaudio.transforms.Resample(44100, 48000, resampling_method='kaiser_window')\n",
        "device = 'cuda:0'\n",
        "Resample = Resample.to(device)\n",
        "\n",
        "from pathlib import Path\n",
        "import random\n",
        "import numpy as np\n",
        "from scipy import interpolate as sp_interpolate\n",
        "import json\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "import soundfile as sf\n",
        "# import sounddevice as sd\n",
        "import configparser\n",
        "import random\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zR4gkZoSsrbG"
      },
      "outputs": [],
      "source": [
        "sampling_rate = 44100\n",
        "sr = sampling_rate\n",
        "\n",
        "hop_length = 128\n",
        "\n",
        "segment_length = 1024\n",
        "n_units = 2048\n",
        "latent_dim = 256\n",
        "device = 'cuda:0'\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "audio_fold = Path(r'./content/2022-zkm-workshop/ltsp/erokia/audio')\n",
        "audio = audio_fold\n",
        "lts_audio_files = [f for f in audio_fold.glob('*.wav')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSInHDvAu2_-",
        "outputId": "ebab8eac-e201-4b43-874c-ca92bc0ba06b"
      },
      "outputs": [],
      "source": [
        "# Following should give you more than 0\n",
        "\n",
        "len(lts_audio_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdUWpV22trrp"
      },
      "outputs": [],
      "source": [
        "# Models \n",
        "\n",
        "class raw_VAE(nn.Module):\n",
        "  def __init__(self, segment_length, n_units, latent_dim):\n",
        "    super(raw_VAE, self).__init__()\n",
        "\n",
        "    self.segment_length = segment_length\n",
        "    self.n_units = n_units\n",
        "    self.latent_dim = latent_dim\n",
        "    \n",
        "    self.fc1 = nn.Linear(segment_length, n_units)\n",
        "    self.fc21 = nn.Linear(n_units, latent_dim)\n",
        "    self.fc22 = nn.Linear(n_units, latent_dim)\n",
        "    self.fc3 = nn.Linear(latent_dim, n_units)\n",
        "    self.fc4 = nn.Linear(n_units, segment_length)\n",
        "\n",
        "  def encode(self, x):\n",
        "      h1 = F.relu(self.fc1(x))\n",
        "      return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "  def reparameterize(self, mu, logvar):\n",
        "      std = torch.exp(0.5*logvar)\n",
        "      eps = torch.randn_like(std)\n",
        "      return mu + eps*std\n",
        "\n",
        "  def decode(self, z):\n",
        "      h3 = F.relu(self.fc3(z))\n",
        "      return F.tanh(self.fc4(h3))\n",
        "\n",
        "  def forward(self, x):\n",
        "      mu, logvar = self.encode(x.view(-1, self.segment_length))\n",
        "      z = self.reparameterize(mu, logvar)\n",
        "      return self.decode(z), mu, logvar\n",
        "\n",
        "# Reconstruction + KL divergence losses summed over all elements and batch\n",
        "def loss_function(recon_x, x, mu, logvar, kl_beta, segment_length):\n",
        "  recon_loss = F.mse_loss(recon_x, x.view(-1, segment_length))\n",
        "\n",
        "  # see Appendix B from VAE paper:\n",
        "  # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
        "  # https://arxiv.org/abs/1312.6114\n",
        "  # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "  KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "  return recon_loss + ( kl_beta * KLD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6auOc8Zqtx-r"
      },
      "outputs": [],
      "source": [
        "# Datasets \n",
        "\n",
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    This is the main class that calculates the spectrogram and returns the\n",
        "    spectrogram, audio pair.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, audio_np, segment_length, sampling_rate, hop_size, transform=None):\n",
        "        \n",
        "        self.transform = transform\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.segment_length = segment_length\n",
        "        self.hop_size = hop_size\n",
        "        \n",
        "        if segment_length % hop_size != 0:\n",
        "            raise ValueError(\"segment_length {} is not a multiple of hop_size {}\".format(segment_length, hop_size))\n",
        "\n",
        "        if len(audio_np) % hop_size != 0:\n",
        "            num_zeros = hop_size - (len(audio_np) % hop_size)\n",
        "            audio_np = np.pad(audio_np, (0, num_zeros), 'constant', constant_values=(0,0))\n",
        "\n",
        "        self.audio_np = audio_np\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        # Take segment\n",
        "        seg_start = index * self.hop_size\n",
        "        seg_end = (index * self.hop_size) + self.segment_length\n",
        "        sample = self.audio_np[ seg_start : seg_end ]\n",
        "        \n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.audio_np) // self.hop_size) - (self.segment_length // self.hop_size) + 1\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        return torch.from_numpy(sample)\n",
        "\n",
        "class TestDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    This is the main class that calculates the spectrogram and returns the\n",
        "    spectrogram, audio pair.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, audio_np, segment_length, sampling_rate, transform=None):\n",
        "        \n",
        "        self.transform = transform\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.segment_length = segment_length\n",
        "        \n",
        "        if len(audio_np) % segment_length != 0:\n",
        "            num_zeros = segment_length - (len(audio_np) % segment_length)\n",
        "            audio_np = np.pad(audio_np, (0, num_zeros), 'constant', constant_values=(0,0))\n",
        "\n",
        "        self.audio_np = audio_np\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        # Take segment\n",
        "        seg_start = index * self.segment_length\n",
        "        seg_end = (index * self.segment_length) + self.segment_length\n",
        "        sample = self.audio_np[ seg_start : seg_end ]\n",
        "        \n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_np) // self.segment_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAK-ZayJs37R",
        "outputId": "78b63eea-6f03-4ea1-dff1-8217a12ad6a8"
      },
      "outputs": [],
      "source": [
        "state = torch.load(Path(r'./content/2022-zkm-workshop/nospectral/erokia/spectralvae/run-000/checkpoints/ckpt_00500'))\n",
        "raw_model = raw_VAE(segment_length, n_units, latent_dim).to(device)\n",
        "raw_model.load_state_dict(state['state_dict'])\n",
        "raw_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yvd-dG9ZQ3du"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_0X8tm1s6Yp"
      },
      "source": [
        "## Stepwise Interpolations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKib2gtKtRd6"
      },
      "outputs": [],
      "source": [
        "# Load audio files ...\n",
        "\n",
        "test_audio_1_path = lts_audio_files[random.randint(0, len(lts_audio_files) - 1)]\n",
        "test_audio_1, fs = librosa.load(test_audio_1_path, sr=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        },
        "id": "EWc6rIvltRd8",
        "outputId": "834b5e20-c0e7-43be-a261-1c58df7b1078"
      },
      "outputs": [],
      "source": [
        "display.Audio(test_audio_1, rate=fs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JRi7WzdtRd8"
      },
      "outputs": [],
      "source": [
        "test_audio_2_path = lts_audio_files[random.randint(0, len(lts_audio_files)- 1)]\n",
        "test_audio_2, fs = librosa.load(test_audio_2_path, sr=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        },
        "id": "LbZCfyFjtRd9",
        "outputId": "f202fffc-30ba-4304-b12a-93ce0f3d93b6"
      },
      "outputs": [],
      "source": [
        "display.Audio(test_audio_2, rate=fs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZ7lFDlBtheZ"
      },
      "outputs": [],
      "source": [
        "# We should match the audio lengths\n",
        "# 0 is crop the longer, \n",
        "# 1 is repeat the shorter, \n",
        "#  \n",
        "\n",
        "# TODO make this an array/tensor of two - check if padding functions have these \n",
        "\n",
        "match_size = 1\n",
        "\n",
        "if match_size == 0:\n",
        "    if test_audio_1.shape[0] < test_audio_2.shape[0]:\n",
        "        test_audio_2 = test_audio_2[:test_audio_1.shape[0]]\n",
        "    else:\n",
        "        test_audio_1 = test_audio_1[:test_audio_2.shape[0]]\n",
        "\n",
        "if match_size == 1:\n",
        "    if test_audio_1.shape[0] < test_audio_2.shape[0]:\n",
        "        while test_audio_1.shape[0] < test_audio_2.shape[0]:\n",
        "            test_audio_1 = np.concatenate((test_audio_1, test_audio_1), 0)\n",
        "        test_audio_1 = test_audio_1[:test_audio_2.shape[0]] \n",
        "\n",
        "    else:\n",
        "        while test_audio_2.shape[0] < test_audio_1.shape[0]:\n",
        "            test_audio_2 = np.concatenate((test_audio_2, test_audio_2), 0)\n",
        "        test_audio_2 = test_audio_2[:test_audio_1.shape[0]]\n",
        "\n",
        "# Create the dataset\n",
        "test_dataset1 = TestDataset(test_audio_1, segment_length = segment_length, sampling_rate = sampling_rate, transform=ToTensor())\n",
        "test_dataset2 = TestDataset(test_audio_2, segment_length = segment_length, sampling_rate = sampling_rate, transform=ToTensor())\n",
        "\n",
        "test_dataloader1 = DataLoader(test_dataset1, batch_size = batch_size, shuffle=False)\n",
        "test_dataloader2 = DataLoader(test_dataset2, batch_size = batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jbNJP14tlfp"
      },
      "outputs": [],
      "source": [
        "def raw_to_z_dist(test_dataloader, raw_model, device):\n",
        "    init_test = True\n",
        "    for iterno, test_sample in enumerate(test_dataloader):\n",
        "        with torch.no_grad():\n",
        "            test_sample = test_sample.to(device)\n",
        "            test_mu, test_logvar = raw_model.encode(test_sample)\n",
        "\n",
        "        if init_test:\n",
        "            test_z_mu = test_mu \n",
        "            test_z_logvar = test_logvar\n",
        "            init_test = False\n",
        "\n",
        "        else:\n",
        "            test_z_mu = torch.cat((test_z_mu, test_mu ),0)\n",
        "            test_z_logvar = torch.cat((test_z_logvar, test_logvar ),0)\n",
        "    return test_z_mu, test_z_logvar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HU3q9kLMRJkW"
      },
      "outputs": [],
      "source": [
        "# Z dist\n",
        "\n",
        "test1_z_mu, test1_z_logvar = raw_to_z_dist(test_dataloader1, raw_model, device)\n",
        "test2_z_mu, test2_z_logvar = raw_to_z_dist(test_dataloader2, raw_model, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRwJH7BPvpGc"
      },
      "outputs": [],
      "source": [
        "def raw_interpolate_stepwise_z_dist(test1_z_mu, test1_z_logvar, test2_z_mu, test2_z_logvar, interpolation_range, raw_model):\n",
        "\n",
        "    init_test = True\n",
        "    for interpolation in interpolation_range:\n",
        "\n",
        "        inter_z_mu = torch.add( torch.mul(test1_z_mu, (1-interpolation)), torch.mul(test2_z_mu, interpolation) )\n",
        "        inter_z_logvar = torch.add( torch.mul(test1_z_logvar, (1-interpolation)), torch.mul(test2_z_logvar, interpolation) )\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            test_pred_z = raw_model.reparameterize(inter_z_mu, inter_z_logvar)\n",
        "            test_pred = raw_model.decode(test_pred_z)\n",
        "\n",
        "        if init_test:\n",
        "            test_predictions = test_pred\n",
        "            init_test = False\n",
        "\n",
        "        else:\n",
        "            test_predictions = torch.cat((test_predictions, test_pred ),0)\n",
        "        \n",
        "    return test_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cl7AjpGTtpn5",
        "outputId": "497d55c0-5d55-4a74-e862-b43b0a119bc2"
      },
      "outputs": [],
      "source": [
        "interpolation_range = np.arange(0, 1.1, 0.2)\n",
        "\n",
        "inter_raw_all = raw_interpolate_stepwise_z_dist(test1_z_mu, test1_z_logvar, test2_z_mu, test2_z_logvar, interpolation_range, raw_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbH4ZKsxSOhS"
      },
      "outputs": [],
      "source": [
        "inter_raw_all_np = inter_raw_all.view(-1).cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        },
        "id": "e4OOtZOoRYZK",
        "outputId": "d9d3f80b-ad3d-4cfb-ac3e-07319471d014"
      },
      "outputs": [],
      "source": [
        "display.Audio(inter_raw_all_np, rate=fs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuaPEY4UwDVr"
      },
      "outputs": [],
      "source": [
        "out_path  = 'my_audio.wav'\n",
        "sf.write(out_path, inter_raw_all_np, sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "ZVWDsXTYRf4h",
        "outputId": "5fb28350-6be5-40cf-d05f-384b7f0713e5"
      },
      "outputs": [],
      "source": [
        "librosa.display.waveshow(inter_raw_all_np, sr=sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "JyCbRsRTSKx6",
        "outputId": "b65755a4-1111-407c-9dfb-c5c44bae6f2c"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)\n",
        "D = librosa.amplitude_to_db(np.abs(librosa.stft(inter_raw_all_np, hop_length=hop_length)),ref=np.max)\n",
        "img = librosa.display.specshow(D, y_axis='log', sr=sr, hop_length=hop_length, x_axis='time', ax=ax)\n",
        "ax.set(title='Log-frequency power spectrogram')\n",
        "ax.label_outer()\n",
        "fig.colorbar(img, ax=ax, format=\"%+2.f dB\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WIxgNZ6SUkpa"
      },
      "source": [
        "\n",
        "## Interpolations in Meso-scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gdrg7pSxyz1"
      },
      "outputs": [],
      "source": [
        "def concat_random_audio(audio_files, duration, sampling_rate):\n",
        "    \n",
        "    # concat random files from the dataset until the array is [duration] secs long.\n",
        "\n",
        "    array_length = 0\n",
        "\n",
        "    init = True\n",
        "\n",
        "    while array_length < (duration * sampling_rate):\n",
        "        \n",
        "        path = audio_files[random.randint(0, len(audio_files)- 1)]\n",
        "        \n",
        "        y, fs = librosa.load(path, sr=None)\n",
        "        \n",
        "        if init:\n",
        "            audio_1 = y\n",
        "        else:\n",
        "            audio_1 = np.concatenate((audio_1, y), 0)\n",
        "\n",
        "        init = False\n",
        "        array_length = audio_1.shape[0]\n",
        "\n",
        "    init = True\n",
        "    array_length = 0\n",
        "\n",
        "    while array_length < (duration * sampling_rate):\n",
        "        \n",
        "        path = audio_files[random.randint(0, len(audio_files)- 1)]\n",
        "        \n",
        "        y, fs = librosa.load(path, sr=None)\n",
        "        \n",
        "        if init:\n",
        "            audio_2 = y\n",
        "        else:\n",
        "            audio_2 = np.concatenate((audio_2, y), 0)\n",
        "\n",
        "        init = False\n",
        "        array_length = audio_2.shape[0]\n",
        "\n",
        "    return audio_1[:duration * sampling_rate], audio_2[:duration * sampling_rate]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y03M2WMR2DWp"
      },
      "outputs": [],
      "source": [
        "audio_1, audio_2 = concat_random_audio(lts_audio_files, duration=120, sampling_rate=sampling_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "5Cidiaa02E1P",
        "outputId": "31e7e2d7-e3dd-4c01-aeec-ab7c5f955844"
      },
      "outputs": [],
      "source": [
        "librosa.display.waveshow(audio_1, sr=sr)\n",
        "display.Audio(audio_1, rate=sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "Qs9H5Qu72Gd6",
        "outputId": "2033f20a-7071-4a4a-de87-7f5eff672a77"
      },
      "outputs": [],
      "source": [
        "librosa.display.waveshow(audio_2, sr=sr)\n",
        "display.Audio(audio_2, rate=sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9x1vinlc2Vfm"
      },
      "outputs": [],
      "source": [
        "som_clusters_path = r\"./content/2022-zkm-workshop/ltsp/erokia/som/clusters.json\"\n",
        "som_data_path = r\"./content/2022-zkm-workshop/ltsp/erokia/som/data-concatenated.json\"\n",
        "\n",
        "with open(som_clusters_path, 'r') as f:\n",
        "  som_clusters_dict = json.load(f)\n",
        "\n",
        "with open(som_data_path, 'r') as f:\n",
        "  som_data_dict = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9niyWqe2awH"
      },
      "outputs": [],
      "source": [
        "def concat_audio_som(audio_files, sampling_rate, cluster_idx, som_clusters_dict, som_data_dict):\n",
        "    init = True\n",
        "    cluster = som_clusters_dict[str(cluster_idx)]\n",
        "\n",
        "    for index in cluster:\n",
        "    \n",
        "        path = som_data_dict[str(index)][1]\n",
        "        path = audio_files.joinpath(path)\n",
        "        y, fs = librosa.load(path, sr=None)\n",
        "        \n",
        "        if init:\n",
        "            audio = y\n",
        "        else:\n",
        "            audio = np.concatenate((audio, y), 0)\n",
        "\n",
        "        init = False\n",
        "\n",
        "    return audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KzpLPHRUdNP",
        "outputId": "7a261145-d3d2-42d0-e2cb-5566e9441381"
      },
      "outputs": [],
      "source": [
        "cluster_idx = 18\n",
        "test_audio_1 = concat_audio_som(audio, sampling_rate, cluster_idx, som_clusters_dict, som_data_dict)\n",
        "len(test_audio_1) / sampling_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKHlXwoLXS38"
      },
      "outputs": [],
      "source": [
        "librosa.display.waveshow(test_audio_1, sr=sr)\n",
        "display.Audio(test_audio_1, rate=sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQL68pykXPMB",
        "outputId": "fc39a136-8218-4819-f1d7-d69f697aa3a0"
      },
      "outputs": [],
      "source": [
        "cluster_idx = 24\n",
        "test_audio_2 = concat_audio_som(audio, sampling_rate, cluster_idx, som_clusters_dict, som_data_dict)\n",
        "len(test_audio_2) / sampling_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "s9wpQw1fXPoC",
        "outputId": "e9aaef20-e863-49ea-ddb6-a052bf97a44c"
      },
      "outputs": [],
      "source": [
        "librosa.display.waveshow(test_audio_2, sr=sr)\n",
        "display.Audio(test_audio_2, rate=sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkYW-PP427_5"
      },
      "outputs": [],
      "source": [
        "def match_audio_size(audio_1, audio_2, match_size = 1):\n",
        "    # We should match the audio lengths\n",
        "    # 0 is crop the longer, \n",
        "    # 1 is repeat the shorter, \n",
        "    #  \n",
        "\n",
        "    match_size = 1\n",
        "\n",
        "    if match_size == 0:\n",
        "        if audio_1.shape[0] < audio_2.shape[0]:\n",
        "            audio_2 = audio_2[:audio_1.shape[0]]\n",
        "        else:\n",
        "            audio_1 = audio_1[:audio_2.shape[0]]\n",
        "\n",
        "    if match_size == 1:\n",
        "        if audio_1.shape[0] < audio_2.shape[0]:\n",
        "            while audio_1.shape[0] < audio_2.shape[0]:\n",
        "                audio_1 = np.concatenate((audio_1, audio_1), 0)\n",
        "            audio_1 = audio_1[:audio_2.shape[0]] \n",
        "\n",
        "        else:\n",
        "            while audio_2.shape[0] < audio_1.shape[0]:\n",
        "                audio_2 = np.concatenate((audio_2, audio_2), 0)\n",
        "            audio_2 = audio_2[:audio_1.shape[0]]\n",
        "    return audio_1, audio_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fo3y4CbEXWVP"
      },
      "outputs": [],
      "source": [
        "test_audio_1, test_audio_2 = match_audio_size(test_audio_1, test_audio_2, match_size = 1)\n",
        "\n",
        "long_dataset1 = TestDataset(test_audio_1, segment_length = segment_length, sampling_rate = sampling_rate, transform=ToTensor())\n",
        "long_dataloader1 = DataLoader(long_dataset1, batch_size = batch_size, shuffle=False)\n",
        "long1_z_mu, long1_z_logvar = raw_to_z_dist(long_dataloader1, raw_model, device)\n",
        "\n",
        "long_dataset2 = TestDataset(test_audio_2, segment_length = segment_length, sampling_rate = sampling_rate, transform=ToTensor())\n",
        "long_dataloader2 = DataLoader(long_dataset2, batch_size = batch_size, shuffle=False)\n",
        "long2_z_mu, long2_z_logvar = raw_to_z_dist(long_dataloader2, raw_model, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ksv2NsnXYM_"
      },
      "outputs": [],
      "source": [
        "# interpolation = np.sin(np.linspace(-10* np.pi, 10 * np.pi, 20000))\n",
        "interpolation = np.sin(np.linspace(-500* np.pi, 500* np.pi, 20000))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvZ6tskOXZJq",
        "outputId": "70a36781-5593-43a7-f835-d887c9a230a0"
      },
      "outputs": [],
      "source": [
        "  \n",
        "# match the size of interpolation array to audio array using scipy interpolation \n",
        "f_stretch = sp_interpolate.interp1d(np.arange(0, len(interpolation)), interpolation)\n",
        "my_stretched_alfa = f_stretch(np.linspace(0.0, len(interpolation)-1, len(long1_z_mu)))\n",
        "my_stretched_alfa = torch.from_numpy(my_stretched_alfa).to(device)\n",
        "\n",
        "# repeat the alfa values to 256 latent dimensions, that is, each alfa value is a constant multiplier for a latent vector\n",
        "my_stretched_alfa = torch.repeat_interleave(my_stretched_alfa.unsqueeze(1), long1_z_mu.shape[1], axis=1)\n",
        "\n",
        "#generate mixed latent vectors    \n",
        "#alfa(latent1-latent2)+latent2 = alfa * latent1 + (1-alfa) * latent2\n",
        "inter_z_mu = torch.add( torch.mul(long1_z_mu, (1-my_stretched_alfa)), torch.mul(long2_z_mu, my_stretched_alfa) )\n",
        "inter_z_logvar = torch.add( torch.mul(long1_z_logvar, (1-my_stretched_alfa)), torch.mul(long2_z_logvar, my_stretched_alfa) )\n",
        "\n",
        "init_test = True\n",
        "with torch.no_grad():\n",
        "     \n",
        "    test_pred_z = raw_model.reparameterize(inter_z_mu, inter_z_logvar)\n",
        "    test_pred = raw_model.decode(test_pred_z.float())\n",
        "\n",
        "    if init_test:\n",
        "        test_predictions = test_pred\n",
        "        init_test = False\n",
        "\n",
        "    else:\n",
        "        test_predictions = torch.cat((test_predictions, test_pred ),0)\n",
        "\n",
        "out_audio = test_predictions.view(-1).cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMhNTa1mXiKY"
      },
      "outputs": [],
      "source": [
        "import soundfile as sf\n",
        "\n",
        "out_path  = '/content/normal.wav'\n",
        "sf.write(out_path, out_audio, sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "15xKk6bt_uXG",
        "outputId": "c0f60595-dc19-4e90-8b45-3f7b60b34cc8"
      },
      "outputs": [],
      "source": [
        "librosa.display.waveshow(out_audio, sr=sr)\n",
        "display.Audio(out_audio, rate=sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNnr7YA-SfS4"
      },
      "source": [
        "## Interpolations with Extensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pa6qnEbvSiyd"
      },
      "outputs": [],
      "source": [
        "def concat_random_audio(audio_files, duration, sampling_rate):\n",
        "    \n",
        "    # concat random files from the dataset until the array is [duration] secs long.\n",
        "\n",
        "    array_length = 0\n",
        "\n",
        "    init = True\n",
        "\n",
        "    while array_length < (duration * sampling_rate):\n",
        "        \n",
        "        path = audio_files[random.randint(0, len(audio_files)- 1)]\n",
        "        \n",
        "        y, fs = librosa.load(path, sr=None)\n",
        "        \n",
        "        if init:\n",
        "            audio_1 = y\n",
        "        else:\n",
        "            audio_1 = np.concatenate((audio_1, y), 0)\n",
        "\n",
        "        init = False\n",
        "        array_length = audio_1.shape[0]\n",
        "\n",
        "    init = True\n",
        "    array_length = 0\n",
        "\n",
        "    while array_length < (duration * sampling_rate):\n",
        "        \n",
        "        path = audio_files[random.randint(0, len(audio_files)- 1)]\n",
        "        \n",
        "        y, fs = librosa.load(path, sr=None)\n",
        "        \n",
        "        if init:\n",
        "            audio_2 = y\n",
        "        else:\n",
        "            audio_2 = np.concatenate((audio_2, y), 0)\n",
        "\n",
        "        init = False\n",
        "        array_length = audio_2.shape[0]\n",
        "\n",
        "    return audio_1[:duration * sampling_rate], audio_2[:duration * sampling_rate]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjW40ptnSWMs"
      },
      "outputs": [],
      "source": [
        "audio_1, audio_2 = concat_random_audio(lts_audio_files, duration=120, sampling_rate=sampling_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "HzFOkg9nSxsv",
        "outputId": "51e46b0f-ecb7-465a-cae1-853d41f71b0b"
      },
      "outputs": [],
      "source": [
        "librosa.display.waveshow(audio_1, sr=sr)\n",
        "display.Audio(audio_1, rate=sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "iDUI1Rq0SxUg",
        "outputId": "bd6704cc-216c-47cb-eff8-5ebf39570a32"
      },
      "outputs": [],
      "source": [
        "librosa.display.waveshow(audio_2, sr=sr)\n",
        "display.Audio(audio_2, rate=sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJb3A1ROSsVU"
      },
      "outputs": [],
      "source": [
        "def concat_audio_som(audio_files, sampling_rate, cluster_idx, som_clusters_dict, som_data_dict):\n",
        "    init = True\n",
        "    cluster = som_clusters_dict[str(cluster_idx)]\n",
        "\n",
        "    for index in cluster:\n",
        "    \n",
        "        path = som_data_dict[str(index)][1]\n",
        "        path = audio_files.joinpath(path)\n",
        "        y, fs = librosa.load(path, sr=None)\n",
        "        \n",
        "        if init:\n",
        "            audio = y\n",
        "        else:\n",
        "            audio = np.concatenate((audio, y), 0)\n",
        "\n",
        "        init = False\n",
        "\n",
        "    return audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sq7H5Mg6Svhf"
      },
      "outputs": [],
      "source": [
        "som_clusters_path = r\"./content/2022-zkm-workshop/ltsp/erokia/som/clusters.json\"\n",
        "som_data_path = r\"./content/2022-zkm-workshop/ltsp/erokia/som/data-concatenated.json\"\n",
        "\n",
        "with open(som_clusters_path, 'r') as f:\n",
        "  som_clusters_dict = json.load(f)\n",
        "\n",
        "with open(som_data_path, 'r') as f:\n",
        "  som_data_dict = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3knyNwnTYEf",
        "outputId": "6323c526-b12e-43c8-9ce4-96abce6b0822"
      },
      "outputs": [],
      "source": [
        "cluster_idx = 32\n",
        "test_audio_1 = concat_audio_som(audio, sampling_rate, cluster_idx, som_clusters_dict, som_data_dict)\n",
        "len(test_audio_1) / sampling_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltPMbeWoTdse",
        "outputId": "f0a2573d-871d-44a5-fa6e-cbd4a11252b8"
      },
      "outputs": [],
      "source": [
        "cluster_idx = 46\n",
        "test_audio_2 = concat_audio_som(audio, sampling_rate, cluster_idx, som_clusters_dict, som_data_dict)\n",
        "len(test_audio_2) / sampling_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "zJgPJ-4iTe7j",
        "outputId": "965dd989-23e8-4e58-9f58-7f7d551c67af"
      },
      "outputs": [],
      "source": [
        "librosa.display.waveshow(test_audio_1, sr=sr)\n",
        "display.Audio(test_audio_1, rate=sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "z9R5xjpPTkNw",
        "outputId": "39f464f8-588b-4a96-eaba-3bcb35bcf43d"
      },
      "outputs": [],
      "source": [
        "librosa.display.waveshow(test_audio_2, sr=sr)\n",
        "display.Audio(test_audio_2, rate=sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iL9KPIyvUHYf"
      },
      "outputs": [],
      "source": [
        "def match_audio_size(audio_1, audio_2, match_size = 1):\n",
        "    # We should match the audio lengths\n",
        "    # 0 is crop the longer, \n",
        "    # 1 is repeat the shorter, \n",
        "    #  \n",
        "\n",
        "    match_size = 1\n",
        "\n",
        "    if match_size == 0:\n",
        "        if audio_1.shape[0] < audio_2.shape[0]:\n",
        "            audio_2 = audio_2[:audio_1.shape[0]]\n",
        "        else:\n",
        "            audio_1 = audio_1[:audio_2.shape[0]]\n",
        "\n",
        "    if match_size == 1:\n",
        "        if audio_1.shape[0] < audio_2.shape[0]:\n",
        "            while audio_1.shape[0] < audio_2.shape[0]:\n",
        "                audio_1 = np.concatenate((audio_1, audio_1), 0)\n",
        "            audio_1 = audio_1[:audio_2.shape[0]] \n",
        "\n",
        "        else:\n",
        "            while audio_2.shape[0] < audio_1.shape[0]:\n",
        "                audio_2 = np.concatenate((audio_2, audio_2), 0)\n",
        "            audio_2 = audio_2[:audio_1.shape[0]]\n",
        "    return audio_1, audio_2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvlQsLAjTmiD"
      },
      "outputs": [],
      "source": [
        "test_audio_1, test_audio_2 = match_audio_size(test_audio_1, test_audio_2, match_size = 1)\n",
        "\n",
        "long_dataset1 = AudioDataset(test_audio_1, segment_length = segment_length, sampling_rate = sampling_rate, hop_size = hop_length, transform=ToTensor())\n",
        "long_dataloader1 = DataLoader(long_dataset1, batch_size = batch_size, shuffle=False)\n",
        "long1_z_mu, long1_z_logvar = raw_to_z_dist(long_dataloader1, raw_model, device)\n",
        "\n",
        "long_dataset2 = AudioDataset(test_audio_2, segment_length = segment_length, sampling_rate = sampling_rate, hop_size = hop_length, transform=ToTensor())\n",
        "long_dataloader2 = DataLoader(long_dataset2, batch_size = batch_size, shuffle=False)\n",
        "long2_z_mu, long2_z_logvar = raw_to_z_dist(long_dataloader2, raw_model, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjb1TIkkUJ7D"
      },
      "outputs": [],
      "source": [
        "interpolation = np.sin(np.linspace(-np.pi, np.pi, 2000))\n",
        "# interpolation = np.clip(interpolation * 10, -1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emDMbwWMUO9l",
        "outputId": "6be42896-aa7c-4f88-dcab-c8008f555107"
      },
      "outputs": [],
      "source": [
        "  \n",
        "# match the size of interpolation array to audio array using scipy interpolation \n",
        "f_stretch = sp_interpolate.interp1d(np.arange(0, len(interpolation)), interpolation)\n",
        "my_stretched_alfa = f_stretch(np.linspace(0.0, len(interpolation)-1, len(long1_z_mu)))\n",
        "my_stretched_alfa = torch.from_numpy(my_stretched_alfa).to(device)\n",
        "\n",
        "# repeat the alfa values to 256 latent dimensions, that is, each alfa value is a constant multiplier for a latent vector\n",
        "my_stretched_alfa = torch.repeat_interleave(my_stretched_alfa.unsqueeze(1), long1_z_mu.shape[1], axis=1)\n",
        "\n",
        "#generate mixed latent vectors    \n",
        "#alfa(latent1-latent2)+latent2 = alfa * latent1 + (1-alfa) * latent2\n",
        "inter_z_mu = torch.add( torch.mul(long1_z_mu, (1-my_stretched_alfa)), torch.mul(long2_z_mu, my_stretched_alfa) )\n",
        "inter_z_logvar = torch.add( torch.mul(long1_z_logvar, (1-my_stretched_alfa)), torch.mul(long2_z_logvar, my_stretched_alfa) )\n",
        "\n",
        "init_test = True\n",
        "with torch.no_grad():\n",
        "     \n",
        "    test_pred_z = raw_model.reparameterize(inter_z_mu, inter_z_logvar)\n",
        "    test_pred = raw_model.decode(test_pred_z.float())\n",
        "\n",
        "    if init_test:\n",
        "        test_predictions = test_pred\n",
        "        init_test = False\n",
        "\n",
        "    else:\n",
        "        test_predictions = torch.cat((test_predictions, test_pred ),0)\n",
        "\n",
        "\n",
        "out_audio = test_predictions.view(-1).cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcdU7_OUWKLN"
      },
      "outputs": [],
      "source": [
        "import soundfile as sf\n",
        "\n",
        "out_path  = 'hacky.wav'\n",
        "sf.write(out_path, out_audio, sr)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Acknowledgements\n",
        "\n",
        "This code is built within the following research residency:\n",
        "\n",
        "https://kivanctatar.com/Coding-Latent-No-1\n",
        "\n",
        "This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program â€“ Humanities and Society (WASP-HS) funded by the Marianne and Marcus Wallenberg Foundation and the Marcus and Amalia Wallenberg Foundation."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "GLhSNjjgERrr",
        "VaOjyqxHg-FP",
        "vzmoFaAunkIx",
        "eH70BnNZoD2V"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
